{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Content Analytics - AUEB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "from pandas import read_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The device came back in a worse condition when...</td>\n",
       "      <td>Repair Experience/Availability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reply</td>\n",
       "      <td>Support Efficiency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Your employees are always friendly. However, o...</td>\n",
       "      <td>Support Efficiency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Been told the correct information to start wit...</td>\n",
       "      <td>Information Provided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By having better service support someone who u...</td>\n",
       "      <td>Information Provided</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  \\\n",
       "0  The device came back in a worse condition when...   \n",
       "1                                              reply   \n",
       "2  Your employees are always friendly. However, o...   \n",
       "3  Been told the correct information to start wit...   \n",
       "4  By having better service support someone who u...   \n",
       "\n",
       "                         Category  \n",
       "0  Repair Experience/Availability  \n",
       "1              Support Efficiency  \n",
       "2              Support Efficiency  \n",
       "3            Information Provided  \n",
       "4            Information Provided  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/sakis/Desktop/The_Dataset\",sep=\"~\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a basic description, the dataset consists of 27649 comments and each one of them belongs to a category. \n",
    "The dataset consists of no null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27649 entries, 0 to 27648\n",
      "Data columns (total 2 columns):\n",
      "Comment     27649 non-null object\n",
      "Category    27649 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 432.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look in the frequency of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Repair Experience/Availability': 1508,\n",
       "         'Support Efficiency': 5179,\n",
       "         'Information Provided': 3550,\n",
       "         'Warranty Coverage': 1185,\n",
       "         'Product/Spares Quality': 2171,\n",
       "         'Issue Resolution': 4931,\n",
       "         'Agent Contact Skills': 2251,\n",
       "         'Positive Verbatim': 1801,\n",
       "         'Price': 291,\n",
       "         'Promotion Conditions': 371,\n",
       "         'Website / Store Experience': 1933,\n",
       "         '3rd Party Complaint': 163,\n",
       "         'Product /Spare Availability': 2080,\n",
       "         'Other': 217,\n",
       "         'issue Resolution': 1,\n",
       "         'other': 9,\n",
       "         'price': 1,\n",
       "         'positive verbatim': 4,\n",
       "         'issue resolution': 1,\n",
       "         '3rd party complaint': 1,\n",
       "         'promotion conditions': 1})"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counts = Counter(df['Category'])\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a small problem with lower and upper case letters in the \"Category\" column, so we can fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'repair experience/availability': 1508,\n",
       "         'support efficiency': 5179,\n",
       "         'information provided': 3550,\n",
       "         'warranty coverage': 1185,\n",
       "         'product/spares quality': 2171,\n",
       "         'issue resolution': 4933,\n",
       "         'agent contact skills': 2251,\n",
       "         'positive verbatim': 1805,\n",
       "         'price': 292,\n",
       "         'promotion conditions': 372,\n",
       "         'website / store experience': 1933,\n",
       "         '3rd party complaint': 164,\n",
       "         'product /spare availability': 2080,\n",
       "         'other': 226})"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Category\"]= df[\"Category\"].str.lower() \n",
    "category_counts = Counter(df['Category'])\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 of the 14 categories contain a small number of data compared to the other 10 categories. As a first attempt we will keep all the  14 categories and see how well the model responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_categories = category_counts.most_common()[:14]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comments will be the input for the model and the categories will be the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to split the dataset. We are going to split it in 3 components. One will be the training dataset, one the validation dataset and the last one the test dataset. StratifiedShuffleSplit shuffles the data, and then it splits the data into n_splits parts. After this step, StratifiedShuffleSplit picks one part to use as a test set. Then it repeats the same process n_splits - 1 other times, to get n_splits - 1 other test sets. So, it shuffles each time before splitting, and it splits n_splits times, and as a result the test sets can overlap.\n",
    "At first, we will split the Original dataset into two pieces:Train-Validation dataset and Test dataset\n",
    "Secondly, we will split the Train-Validation dataset into another two pieces:Train dataset and Validation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sss = StratifiedShuffleSplit(n_splits=5,\n",
    "                                  test_size=0.15,\n",
    "                                  random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sss = StratifiedShuffleSplit(n_splits=5, \n",
    "                                 test_size=0.2,\n",
    "                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    In this case, everything was fine, but I have ...\n",
       "1        No aftercare or loyalty towards its customers\n",
       "2    The very service from you has been excellent. ...\n",
       "3                               Employ qualified staff\n",
       "4                                Answered the question\n",
       "Name: Comment, dtype: object"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val.head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset the indexes for both the X-train-val and y-train-val in order to break them \n",
    "again into two subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val = X_train_val.reset_index(drop=True)\n",
    "y_train_val = y_train_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split again the train-val dataset into train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = None, None, None, None\n",
    "\n",
    "for train_index, val_index in val_sss.split(X_train_val, y_train_val):\n",
    "    \n",
    "    X_train, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "    y_train, y_val = y_train_val[train_index], y_train_val[val_index] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to encode the labels (categories) using a One-Hot Encoder. \n",
    "At first we run fit_transform on the Training dataand then we use the fitted One-hot-Encoder to transform the rest of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "y_train_enc = y_enc.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "y_val_enc = y_enc.transform(y_val.values.reshape(-1, 1))\n",
    "y_test_enc = y_enc.transform(y_test.values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After constructing the three datasets we can see their shape. The training dataset consists of 18800 rows, while the test and validation datasets consist of 4148 and 4701 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (18800, 14)\n",
      "y_val shape: (4701, 14)\n",
      "y_test shape: (4148, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('y_train shape: {}'.format(y_train_enc.shape))\n",
    "print('y_val shape: {}'.format(y_val_enc.shape))\n",
    "print('y_test shape: {}'.format(y_test_enc.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Approach (BoW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag-of-words model (BoW), is a way of extracting features from text for use in modeling. The approach is very simple and flexible, and can be used in thousands of ways for extracting features from documents. It is a representation of text that describes the occurrence of words within a document. It involves two things: A vocabulary of known words. and a measure of the presence of known words. The model is only concerned with whether known words occur in the document, not where in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    the device came back in a worse condition when...\n",
       "1                                                reply\n",
       "2    your employees are always friendly  however, o...\n",
       "3    been told the correct information to start wit...\n",
       "4    by having better service support someone who u...\n",
       "Name: Comment, dtype: object"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = df['Comment'].str.lower().str.replace('.', ' ')\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will concatenate all the comments into one text and split in into tokens. then we are going to find the most frequent of them. Below we can see the 15 most frequent words in all the comments and how many times each one of them appears in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 41566),\n",
       " ('i', 21916),\n",
       " ('to', 21045),\n",
       " ('a', 18517),\n",
       " ('not', 15027),\n",
       " ('and', 14717),\n",
       " ('was', 11984),\n",
       " ('of', 9739),\n",
       " ('for', 9139),\n",
       " ('that', 8932),\n",
       " ('my', 8489),\n",
       " ('have', 8400),\n",
       " ('it', 8243),\n",
       " ('is', 7948),\n",
       " ('in', 7900)]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = \" \".join(comments)\n",
    "top_words = Counter(all_words.split()).most_common()\n",
    "top_words[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we deal with text problems, stop words removal process is a one of the most important steps in order to have a better input for the models. The term stop words means that they are very common words in a language. these words do not help on most of the problems such as semantic analysis, classification etc. For that, we will print the top 100 most common words in order to pick those that we want to include in our Stop Words List. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '2', 'a', 'about', 'after', 'again', 'all', 'also', 'an', 'and', 'answer', 'are', 'as', 'at', 'back', 'be', 'because', 'been', 'better', 'but', 'buy', 'by', 'can', 'contact', 'could', 'customer', 'device', 'did', 'do', 'even', 'first', 'for', 'from', 'get', 'good', 'had', 'has', 'have', 'help', 'i', 'if', 'in', 'information', 'is', 'it', 'just', 'machine', 'me', 'more', 'my', 'new', 'no', 'not', 'nothing', 'now', 'of', 'on', 'one', 'only', 'or', 'order', 'out', 'part', 'parts', 'philips', 'problem', 'product', 'products', 'question', 'received', 'repair', 'replacement', 'send', 'sent', 'service', 'should', 'so', 'spare', 'still', 'that', 'the', 'then', 'there', 'they', 'this', 'time', 'to', 'very', 'warranty', 'was', 'we', 'were', 'what', 'when', 'which', 'will', 'with', 'would', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "print(sorted([i[0].lower() for i in top_words[:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our stop list will contain most of the words above. In fact we will exclude words that may be important for the model, such as \"good\" or \"problem\". The final list contains the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_stop = ['-', '2', 'a', 'about', 'after', 'again', 'all', 'also', 'an', 'and',\n",
    "           'are', 'as', 'at', 'back', 'be', 'because', 'been',\n",
    "           'buy', 'by', 'can', 'could', 'customer', \n",
    "           'did', 'do', 'even', 'first', 'for', 'from', 'get', 'had', \n",
    "           'has', 'have', 'i', 'if', 'in', 'is', 'it',\n",
    "           'just', 'me', 'more', 'my', 'new', 'no', 'now', 'of', \n",
    "           'on', 'one', 'only', 'or', 'out', 'part', 'parts',\n",
    "           'send', 'sent', 'service', 'should', 'so', \n",
    "           'still', 'than', 'that', 'the', 'then', 'there', 'they', 'this', 'time', \n",
    "           'to', 'very', 'was', 'we', 'were', 'what', 'when', 'which', \n",
    "           'will', 'with', 'would', 'you', 'your']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set the total number of words used for vectorization to 3000. \"comments_vect_counts\" vectorizer will give the number of appearances of each word and \"comments_vect_binary\" vectorizer will return 1 if the word appears and 0 if the word doesn't appear.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 3000\n",
    "\n",
    "comments_vect_counts = CountVectorizer(encoding='utf-8',\n",
    "                                     strip_accents='unicode',\n",
    "                                     lowercase=True,\n",
    "                                     stop_words=el_stop,\n",
    "                                     ngram_range=(1, 1), # unigrams\n",
    "                                     max_features=max_words,\n",
    "                                     binary=False # binary output or full counts. \n",
    "                                     )\n",
    "\n",
    "comments_vect_binary = CountVectorizer(encoding='utf-8',\n",
    "                                     strip_accents='unicode',\n",
    "                                     lowercase=True,\n",
    "                                     stop_words=el_stop,\n",
    "                                     ngram_range=(1, 1), # unigrams\n",
    "                                     max_features=max_words,\n",
    "                                     binary=True # binary output or full counts. \n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we fit the CountVectorizer only on the training dataset and use it to transform the Validation and Test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = comments_vect_counts.fit_transform(X_train.astype(str))\n",
    "X_val_enc = comments_vect_counts.transform(X_val.astype(str))\n",
    "X_test_enc = comments_vect_counts.transform(X_test.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<18800x3000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 203290 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist of all we set values to some variables that will be used for the model. We set Number of Classes for the Y labels to 14, number of epochs to 30, the batch_size of the data that will be fed to the Model when training to 32 and Dropout Rate of the Dropout Layer to 40%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(y_enc.categories_[0])\n",
    "nb_epoch = 30\n",
    "batch_size = 32 \n",
    "dropout_rate = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a sequential model. In sequential models each layer will use as input the output of the former layer added to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Dense(512))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "\n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some information avout the models' layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 512)               1536512   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 14)                7182      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 14)                0         \n",
      "=================================================================\n",
      "Total params: 1,806,350\n",
      "Trainable params: 1,806,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compile the model we will use categorical crossentropy as the loss function, Adam as an optimizer and  \"accuracy\" as ametric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=[\n",
    "        'accuracy'\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now ready to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18800 samples, validate on 4701 samples\n",
      "Epoch 1/30\n",
      " - 25s - loss: 1.5590 - acc: 0.5098 - val_loss: 1.1700 - val_acc: 0.6311\n",
      "Epoch 2/30\n",
      " - 22s - loss: 0.8534 - acc: 0.7384 - val_loss: 1.0682 - val_acc: 0.6841\n",
      "Epoch 3/30\n",
      " - 23s - loss: 0.5281 - acc: 0.8366 - val_loss: 1.1358 - val_acc: 0.7003\n",
      "Epoch 4/30\n",
      " - 19s - loss: 0.3393 - acc: 0.8945 - val_loss: 1.2840 - val_acc: 0.7081\n",
      "Epoch 5/30\n",
      " - 19s - loss: 0.2436 - acc: 0.9252 - val_loss: 1.3944 - val_acc: 0.7088\n",
      "Epoch 6/30\n",
      " - 18s - loss: 0.1926 - acc: 0.9383 - val_loss: 1.5418 - val_acc: 0.7128\n",
      "Epoch 7/30\n",
      " - 18s - loss: 0.1590 - acc: 0.9486 - val_loss: 1.6113 - val_acc: 0.7156\n",
      "Epoch 8/30\n",
      " - 17s - loss: 0.1435 - acc: 0.9540 - val_loss: 1.7328 - val_acc: 0.7169\n",
      "Epoch 9/30\n",
      " - 17s - loss: 0.1257 - acc: 0.9595 - val_loss: 1.7753 - val_acc: 0.7137\n",
      "Epoch 10/30\n",
      " - 17s - loss: 0.1187 - acc: 0.9615 - val_loss: 1.9144 - val_acc: 0.7116\n",
      "Epoch 11/30\n",
      " - 18s - loss: 0.1108 - acc: 0.9651 - val_loss: 1.9744 - val_acc: 0.7156\n",
      "Epoch 12/30\n",
      " - 18s - loss: 0.1050 - acc: 0.9652 - val_loss: 2.0064 - val_acc: 0.7113\n",
      "Epoch 13/30\n",
      " - 17s - loss: 0.1002 - acc: 0.9676 - val_loss: 2.0506 - val_acc: 0.7141\n",
      "Epoch 14/30\n",
      " - 19s - loss: 0.0912 - acc: 0.9699 - val_loss: 2.1093 - val_acc: 0.7075\n",
      "Epoch 15/30\n",
      " - 18s - loss: 0.0922 - acc: 0.9694 - val_loss: 2.1140 - val_acc: 0.7164\n",
      "Epoch 16/30\n",
      " - 17s - loss: 0.0835 - acc: 0.9723 - val_loss: 2.2244 - val_acc: 0.7122\n",
      "Epoch 17/30\n",
      " - 18s - loss: 0.0925 - acc: 0.9697 - val_loss: 2.2488 - val_acc: 0.7079\n",
      "Epoch 18/30\n",
      " - 18s - loss: 0.0875 - acc: 0.9688 - val_loss: 2.2442 - val_acc: 0.7116\n",
      "Epoch 19/30\n",
      " - 17s - loss: 0.0818 - acc: 0.9734 - val_loss: 2.2476 - val_acc: 0.7096\n",
      "Epoch 20/30\n",
      " - 17s - loss: 0.0793 - acc: 0.9745 - val_loss: 2.3551 - val_acc: 0.7109\n",
      "Epoch 21/30\n",
      " - 18s - loss: 0.0762 - acc: 0.9754 - val_loss: 2.2910 - val_acc: 0.7124\n",
      "Epoch 22/30\n",
      " - 17s - loss: 0.0724 - acc: 0.9746 - val_loss: 2.3606 - val_acc: 0.7113\n",
      "Epoch 23/30\n",
      " - 18s - loss: 0.0732 - acc: 0.9763 - val_loss: 2.3841 - val_acc: 0.7107\n",
      "Epoch 24/30\n",
      " - 18s - loss: 0.0696 - acc: 0.9762 - val_loss: 2.4311 - val_acc: 0.7128\n",
      "Epoch 25/30\n",
      " - 17s - loss: 0.0695 - acc: 0.9770 - val_loss: 2.4927 - val_acc: 0.7075\n",
      "Epoch 26/30\n",
      " - 17s - loss: 0.0685 - acc: 0.9770 - val_loss: 2.4931 - val_acc: 0.7069\n",
      "Epoch 27/30\n",
      " - 17s - loss: 0.0717 - acc: 0.9768 - val_loss: 2.4912 - val_acc: 0.7137\n",
      "Epoch 28/30\n",
      " - 17s - loss: 0.0753 - acc: 0.9753 - val_loss: 2.4693 - val_acc: 0.7103\n",
      "Epoch 29/30\n",
      " - 18s - loss: 0.0760 - acc: 0.9762 - val_loss: 2.4853 - val_acc: 0.7111\n",
      "Epoch 30/30\n",
      " - 21s - loss: 0.0683 - acc: 0.9777 - val_loss: 2.5454 - val_acc: 0.7124\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    \n",
    "    X_train_enc,            # features (as dense inputs)\n",
    "    y_train_enc,            # labels\n",
    "    epochs=nb_epoch,        # number of epochs\n",
    "    batch_size=batch_size,  # define batch size\n",
    "    verbose=2,              # the most extended verbose\n",
    "    validation_data=(       \n",
    "        X_val_enc,          # the validation split that we did before\n",
    "        y_val_enc\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has Test accuracy: 71.842 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 2.4343 - acc: 0.7184\n",
      "\n",
      "Test categorical_crossentropy: 2.4342753984889045\n",
      "\n",
      "Test accuracy: 71.842 %\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(\n",
    "    X_test_enc.todense(),    \n",
    "    y_test_enc,              \n",
    "    batch_size=batch_size,  \n",
    "    verbose=2  \n",
    ")\n",
    "\n",
    "print('\\nTest categorical_crossentropy: {}'.format(score[0]))\n",
    "print('\\nTest accuracy: {:.3f} %'.format(score[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use our model to make some predictions. We will try to predict three new comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_comment = \"\"\"\n",
    "you should treat your customers with more respect\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom prediction function \n",
    "def get_one_hot_predictions(pred_probs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_probs = np.max(pred_probs, axis=1)\n",
    "    \n",
    "    # reshaping to (len_of_predicts, 1)\n",
    "    max_probs = max_probs.reshape(max_probs.shape[0], 1)\n",
    "\n",
    "    return np.equal(pred_probs, max_probs).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3000)\n",
      "\n",
      "Probabilities\n",
      "\n",
      "[[7.8205051e-09 9.9996042e-01 5.0657007e-07 7.6840382e-07 3.8032923e-08\n",
      "  1.6107725e-07 9.8454396e-12 1.4838255e-07 4.9604137e-10 1.4961180e-09\n",
      "  1.3154876e-06 3.0278037e-05 6.2538120e-06 7.3398032e-10]]\n",
      "\n",
      "Probabilities One Hot Vector\n",
      "\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Category Prediction:\n",
      "\n",
      "agent contact skills\n"
     ]
    }
   ],
   "source": [
    "# vectorizing comment with Count Vectorizer\n",
    "comments_vect = comments_vect_counts.transform([input_comment])\n",
    "\n",
    "print(comments_vect.shape, end='\\n\\n')\n",
    "\n",
    "comments_pred = model.predict_proba(comments_vect)\n",
    "\n",
    "print('Probabilities', end='\\n\\n')\n",
    "print(comments_pred, end='\\n\\n')\n",
    "\n",
    "\n",
    "comments_pred_hot = get_one_hot_predictions(comments_pred)\n",
    "\n",
    "print('Probabilities One Hot Vector', end='\\n\\n')\n",
    "print(comments_pred_hot, end='\\n\\n')\n",
    "\n",
    "print('Category Prediction:', end='\\n\\n')\n",
    "print(y_enc.inverse_transform(comments_pred_hot)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_comment = \"\"\"\n",
    "my coffee machine was not working properly. you need to replace it\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3000)\n",
      "\n",
      "Probabilities\n",
      "\n",
      "[[8.0171067e-06 3.7088193e-05 1.8261351e-06 1.9022897e-04 2.8009591e-08\n",
      "  3.2227317e-06 2.1956212e-06 1.6302232e-05 2.4610418e-01 3.6110265e-08\n",
      "  7.4970257e-01 1.7178725e-06 3.9324574e-03 1.9985058e-07]]\n",
      "\n",
      "Probabilities One Hot Vector\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "Category Prediction:\n",
      "\n",
      "repair experience/availability\n"
     ]
    }
   ],
   "source": [
    "# vectorizing comment with Count Vectorizer\n",
    "comments_vect = comments_vect_counts.transform([input_comment])\n",
    "\n",
    "print(comments_vect.shape, end='\\n\\n')\n",
    "\n",
    "comments_pred = model.predict_proba(comments_vect)\n",
    "\n",
    "print('Probabilities', end='\\n\\n')\n",
    "print(comments_pred, end='\\n\\n')\n",
    "\n",
    "\n",
    "comments_pred_hot = get_one_hot_predictions(comments_pred)\n",
    "\n",
    "print('Probabilities One Hot Vector', end='\\n\\n')\n",
    "print(comments_pred_hot, end='\\n\\n')\n",
    "\n",
    "print('Category Prediction:', end='\\n\\n')\n",
    "print(y_enc.inverse_transform(comments_pred_hot)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_comment = \"\"\"\n",
    "i had to repeat myself many times because the person i talked to did not understand what i was saying\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3000)\n",
      "\n",
      "Probabilities\n",
      "\n",
      "[[4.5612730e-21 1.0000000e+00 1.0968228e-17 1.9419117e-16 1.6112674e-21\n",
      "  6.2411744e-24 3.9289934e-34 2.7189185e-27 5.9850743e-22 3.8446253e-22\n",
      "  6.9290071e-19 9.5610242e-10 1.3797056e-20 7.4135892e-23]]\n",
      "\n",
      "Probabilities One Hot Vector\n",
      "\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Category Prediction:\n",
      "\n",
      "agent contact skills\n"
     ]
    }
   ],
   "source": [
    "comments_vect = comments_vect_counts.transform([input_comment])\n",
    "\n",
    "print(comments_vect.shape, end='\\n\\n')\n",
    "\n",
    "comments_pred = model.predict_proba(comments_vect)\n",
    "\n",
    "print('Probabilities', end='\\n\\n')\n",
    "print(comments_pred, end='\\n\\n')\n",
    "\n",
    "\n",
    "comments_pred_hot = get_one_hot_predictions(comments_pred)\n",
    "\n",
    "print('Probabilities One Hot Vector', end='\\n\\n')\n",
    "print(comments_pred_hot, end='\\n\\n')\n",
    "\n",
    "print('Category Prediction:', end='\\n\\n')\n",
    "print(y_enc.inverse_transform(comments_pred_hot)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_comment = \"\"\"\n",
    "i had to sent three e mails before you answer\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3000)\n",
      "\n",
      "Probabilities\n",
      "\n",
      "[[1.7313271e-08 2.0742184e-05 1.9657342e-05 4.6481141e-03 5.6552727e-09\n",
      "  3.9227157e-08 8.5542858e-11 3.7096447e-06 2.8665161e-07 5.1006445e-08\n",
      "  2.1594708e-06 9.9450231e-01 2.9879510e-09 8.0292067e-04]]\n",
      "\n",
      "Probabilities One Hot Vector\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "Category Prediction:\n",
      "\n",
      "support efficiency\n"
     ]
    }
   ],
   "source": [
    "comments_vect = comments_vect_counts.transform([input_comment])\n",
    "\n",
    "print(comments_vect.shape, end='\\n\\n')\n",
    "\n",
    "comments_pred = model.predict_proba(comments_vect)\n",
    "\n",
    "print('Probabilities', end='\\n\\n')\n",
    "print(comments_pred, end='\\n\\n')\n",
    "\n",
    "\n",
    "comments_pred_hot = get_one_hot_predictions(comments_pred)\n",
    "\n",
    "print('Probabilities One Hot Vector', end='\\n\\n')\n",
    "print(comments_pred_hot, end='\\n\\n')\n",
    "\n",
    "print('Category Prediction:', end='\\n\\n')\n",
    "print(y_enc.inverse_transform(comments_pred_hot)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the model makes accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
